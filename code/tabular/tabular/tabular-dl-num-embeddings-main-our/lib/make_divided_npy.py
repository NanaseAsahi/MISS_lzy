import pandas as pd
import numpy as np
from pathlib import Path
import random
import os
import json


base_data_path = Path('/home/lzy/data/MISS2/dataset')
default_output_path = Path('/home/lzy/data/MISS2/data')

datasets = ['gas', 'HI', 'News', 'temperature']
missingtypes = ['mcar', 'mar_p', 'mnar_p']
missingrates = [0.1, 0.3, 0.5, 0.7, 0.9]

# random.seed(0)
# np.random.seed(0)
seeds = [0, 149669, 52983]


def identify_cat_features(X: pd.DataFrame) -> list[bool]:
    # å°†ç‰¹å¾åˆ†ä¸ºè¿ç»­å‹å’Œç¦»æ•£å‹

    # è·å–æ¯åˆ—çš„å”¯ä¸€å€¼ä¸ªæ•°å’Œç±»å‹ ç”¨äºåˆ†è¾¨æ˜¯å¦ä¸ºç¦»æ•£å‹ç‰¹å¾
    nunique = X.nunique()
    types = X.dtypes

    cat_indicator = np.zeros(X.shape[1]).astype(bool)

    for col in X.columns:
        # å…ˆé€šè¿‡ç±»å‹ç›´æ¥åˆ¤æ–­ è®¡ç®—é€Ÿåº¦æ›´å¿«
        if  types[col] == 'object' or nunique[col] < 100:
            # cat_indicator[col] = True
            cat_indicator[X.columns.get_loc(col)] = True

    # è·å¾—è¿ç»­ç‰¹å¾å’Œç¦»æ•£ç‰¹å¾çš„åˆ—å    
    cat_columns = X.columns[cat_indicator].tolist()
    con_columns = list(set(X.columns.tolist()) - set(cat_columns))

    cat_idxs = list(np.where(cat_indicator == True)[0])
    con_idxs = list(set(range(len(X.columns))) - set(cat_idxs))

    return cat_idxs, con_idxs, cat_columns, con_columns

def split_train_test_val(X, y, X_num, X_cat, seed=0):
    # csvæ–‡ä»¶ä¸­æœ€åä¸€è¡Œå‡ä¸ºlabel/y
    random.seed(seed)
    np.random.seed(seed)

    X_copy = X.copy()

    X_copy['Set'] = np.random.choice(['train', 'valid', 'test'], p=[.8, .1, .1], size=X_copy.shape[0])
    train_index = X_copy[X_copy['Set'] == 'train'].index
    test_index = X_copy[X_copy['Set'] == 'test'].index
    val_index = X_copy[X_copy['Set'] == 'valid'].index

    result = {}

    if X_cat is not None:
        result['X_cat'] = {
            'train': X_cat[train_index],
            'test': X_cat[test_index],
            'val': X_cat[val_index]
        }

    if X_num is not None:
        result['X_num'] = {
            'train' : X_num[train_index],
            'test' : X_num[test_index],
            'val' : X_num[val_index],
        }

    result['y'] = {
        'train' : y[train_index],
        'test' : y[test_index],
        'val' : y[val_index],
    }
    

    return result, train_index, test_index, val_index


def save_npy(data_dict, output_dir):
    output_dir = Path(output_dir)
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # ä¿å­˜æ‰€æœ‰æ•°æ®æ–‡ä»¶
    for data_type, data in data_dict.items():
        for split, array in data.items():
            file_name = f'{data_type}_{split}.npy'
            file_path = output_dir / file_name
            np.save(file_path, array)
            print(f'ä¿å­˜ {file_name} åˆ° {file_path}')
                

def process_single_dataset(csv_path, output_dir, seed=0):
    """
    å¤„ç†å•ä¸ªCSVæ–‡ä»¶ï¼Œç”Ÿæˆå¯¹åº”çš„npyæ–‡ä»¶
    """
    # è¯»å–CSVæ•°æ®
    data = pd.read_csv(csv_path)
    
    # åˆ†ç¦»ç‰¹å¾å’Œæ ‡ç­¾
    X = data.iloc[:, :-1]  # æ‰€æœ‰åˆ—é™¤äº†æœ€åä¸€åˆ—
    
    # æ‰¾åˆ°æ ‡ç­¾åˆ—
    possible_label_cols = ['label', 'target', 'Labels', 'shares']
    y_col = None
    for col in possible_label_cols:
        if col in data.columns:
            y_col = col
            break
    
    if y_col is None:
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ ‡å‡†çš„æ ‡ç­¾åˆ—åï¼Œä½¿ç”¨æœ€åä¸€åˆ—
        y = data.iloc[:, -1].values
    else:
        y = data[y_col].values
    
    # è¯†åˆ«åˆ†ç±»ç‰¹å¾å’Œè¿ç»­ç‰¹å¾
    cat_idxs, con_idxs, _, _ = identify_cat_features(X)
    
    # å‡†å¤‡æ•°å€¼ç‰¹å¾å’Œåˆ†ç±»ç‰¹å¾æ•°ç»„
    X_num = X.iloc[:, con_idxs].values if con_idxs else None
    X_cat = X.iloc[:, cat_idxs].values if cat_idxs else None
    
    # æŒ‰train4.pyæ–¹å¼åˆ†å‰²æ•°æ®
    split_data, train_idx, valid_idx, test_idx = split_train_test_val(
        X, y, X_num, X_cat, seed=seed
    )
    
    # ä¿å­˜npyæ–‡ä»¶
    save_npy(split_data, output_dir)
     
    # ç”Ÿæˆå¹¶ä¿å­˜info.json
    task_type = determine_task_type(y)
    n_classes = len(np.unique(y)) if task_type != 'regression' else None
     
    info = {
        'task_type': task_type,
        'n_classes': n_classes,
        'seed': seed,
        'n_samples': {
            'train': len(train_idx),
            'val': len(valid_idx), 
            'test': len(test_idx)
        },
        'n_features': {
            'categorical': len(cat_idxs) if cat_idxs else 0,
            'numerical': len(con_idxs) if con_idxs else 0
        }
    }
     
    info_path = Path(output_dir) / 'info.json'
    with open(info_path, 'w') as f:
        json.dump(info, f, indent=4)
     
    print(f"è®­ç»ƒé›†æ ·æœ¬æ•°: {len(train_idx)}")
    print(f"éªŒè¯é›†æ ·æœ¬æ•°: {len(valid_idx)}")
    print(f"æµ‹è¯•é›†æ ·æœ¬æ•°: {len(test_idx)}")
     
    return {
        'train_size': len(train_idx),
        'val_size': len(valid_idx),
        'test_size': len(test_idx),
        'n_num_features': len(con_idxs) if con_idxs else 0,
        'n_cat_features': len(cat_idxs) if cat_idxs else 0
    }



def determine_task_type(y):
    """ç¡®å®šä»»åŠ¡ç±»å‹"""
    unique_values = np.unique(y)
    if len(unique_values) == 2:
        return 'binclass'
    elif len(unique_values) > 2 and y.dtype in ['int64', 'int32', 'object']:
        return 'multiclass'
    else:
        return 'regression'

def main():
    total_files = len(datasets) * len(missingtypes) * len(missingrates)
    current_file = 0
    success_files = 0
    error_files = 0

    print(f"å¼€å§‹æ‰¹é‡ç”Ÿæˆnpyæ–‡ä»¶...")
    print(f"è¾“å…¥è·¯å¾„: {base_data_path}")
    print(f"è¾“å‡ºè·¯å¾„: {default_output_path}")
    print(f"æ€»è®¡: {total_files} ä¸ªæ•°æ®é›†")
    print(f"æ•°æ®é›†: {datasets}")
    print(f"ç¼ºå¤±ç±»å‹: {missingtypes}")
    print(f"ç¼ºå¤±ç‡: {missingrates}")

    for dataset in datasets:
        for missingtype in missingtypes:
            for missingrate in missingrates:
                current_file += 1

                csv_filename = f'{missingtype}_{dataset}_{missingrate}.csv'
                csv_path = base_data_path / dataset / csv_filename
                output_dir = default_output_path / dataset / f'{missingtype}_{missingrate}'

                # è¿›åº¦æ¡
                print(f"\n[{current_file}/{total_files}] ğŸ”„ {csv_filename}")

                if csv_path.exists():
                    try:
                        # ä¸ºæ¯ä¸ªseedç”Ÿæˆå¯¹åº”çš„æ•°æ®åˆ’åˆ†
                        for seed in seeds:
                            # åˆ›å»ºseedç‰¹å®šçš„è¾“å‡ºç›®å½•
                            seed_output_dir = output_dir / f'seed_{seed}'
                            
                            # å°†æ‰€æœ‰æ“ä½œæŠ½è±¡æˆå¯¹å•ä¸ªæ•°æ®é›†çš„æ“ä½œ
                            info = process_single_dataset(csv_path, seed_output_dir, seed=seed)
                            print(f'  seed={seed} ä¿å­˜åˆ° {seed_output_dir}')
                            print(f"    è®­ç»ƒé›†: {info['train_size']}, éªŒè¯é›†: {info['val_size']}, æµ‹è¯•é›†: {info['test_size']}")
                        
                        print(f'æˆåŠŸå¤„ç†ï¼æ‰€æœ‰seedsæ•°æ®å·²ç”Ÿæˆ')
                        success_files += 1
                    
                    except Exception as e:
                        print(f'å¤„ç†å¤±è´¥: {e}')
                        error_files += 1

                else:
                    print(f'âš ï¸  æ–‡ä»¶ä¸å­˜åœ¨: {csv_path}')
                    error_files += 1

    print(f'\nğŸ‰ å¤„ç†å®Œæˆï¼')
    print(f'æˆåŠŸå¤„ç†: {success_files} ä¸ªæ–‡ä»¶')
    print(f'å¤„ç†å¤±è´¥: {error_files} ä¸ªæ–‡ä»¶')
    print(f'æ€»è®¡: {total_files} ä¸ªæ–‡ä»¶')






if __name__ == '__main__':
    main()